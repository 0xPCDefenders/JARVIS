import { LLama, LLamaConfig, LLamaInferenceArguments } from '@llama-node/core';
import { I as ILLM } from '../llm.d-120996aa.js';

declare class LLamaRS implements ILLM<LLama, LLamaConfig, LLamaInferenceArguments, LLamaInferenceArguments, string> {
    instance: LLama;
    load(config: LLamaConfig): void;
    createCompletion(params: LLamaInferenceArguments, callback: (data: {
        token: string;
        completed: boolean;
    }) => void): Promise<boolean>;
    getEmbedding(params: LLamaInferenceArguments): Promise<number[]>;
    getDefaultEmbedding(text: string): Promise<number[]>;
    tokenize(params: string): Promise<number[]>;
}

export { LLamaRS };
