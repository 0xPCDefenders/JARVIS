var __getOwnPropSymbols = Object.getOwnPropertySymbols;
var __hasOwnProp = Object.prototype.hasOwnProperty;
var __propIsEnum = Object.prototype.propertyIsEnumerable;
var __objRest = (source, exclude) => {
  var target = {};
  for (var prop in source)
    if (__hasOwnProp.call(source, prop) && exclude.indexOf(prop) < 0)
      target[prop] = source[prop];
  if (source != null && __getOwnPropSymbols)
    for (var prop of __getOwnPropSymbols(source)) {
      if (exclude.indexOf(prop) < 0 && __propIsEnum.call(source, prop))
        target[prop] = source[prop];
    }
  return target;
};
var __async = (__this, __arguments, generator) => {
  return new Promise((resolve, reject) => {
    var fulfilled = (value) => {
      try {
        step(generator.next(value));
      } catch (e) {
        reject(e);
      }
    };
    var rejected = (value) => {
      try {
        step(generator.throw(value));
      } catch (e) {
        reject(e);
      }
    };
    var step = (x) => x.done ? resolve(x.value) : Promise.resolve(x.value).then(fulfilled, rejected);
    step((generator = generator.apply(__this, __arguments)).next());
  });
};

// src/llm/llama-cpp.ts
import {
  EmbeddingResultType,
  InferenceResultType,
  LLama,
  TokenizeResultType
} from "@llama-node/llama-cpp";
var LLamaCpp = class {
  load(config) {
    const _a = config, { path, enableLogging } = _a, rest = __objRest(_a, ["path", "enableLogging"]);
    this.instance = LLama.load(path, rest, enableLogging);
  }
  createCompletion(params, callback) {
    return __async(this, null, function* () {
      let completed = false;
      const errors = [];
      return new Promise((res, rej) => {
        this.instance.inference(params, (response) => {
          var _a;
          switch (response.type) {
            case InferenceResultType.Data: {
              const data = {
                token: response.data.token,
                completed: !!response.data.completed
              };
              if (data.completed) {
                completed = true;
              }
              callback(data);
              break;
            }
            case InferenceResultType.End: {
              if (errors.length) {
                rej(new Error(errors.join("\n")));
              } else {
                res(completed);
              }
              break;
            }
            case InferenceResultType.Error: {
              errors.push((_a = response.message) != null ? _a : "Unknown Error");
              break;
            }
          }
        });
      });
    });
  }
  getEmbedding(params) {
    return __async(this, null, function* () {
      return new Promise((res, rej) => {
        this.instance.getWordEmbedding(params, (response) => {
          var _a;
          switch (response.type) {
            case EmbeddingResultType.Data:
              res((_a = response.data) != null ? _a : []);
              break;
            case EmbeddingResultType.Error:
              rej(new Error("Unknown Error"));
              break;
          }
        });
      });
    });
  }
  getDefaultEmbedding(text) {
    return __async(this, null, function* () {
      return this.getEmbedding({
        nThreads: 4,
        nTokPredict: 1024,
        topK: 40,
        topP: 0.1,
        temp: 0.1,
        repeatPenalty: 1,
        prompt: text
      });
    });
  }
  tokenize(params) {
    return __async(this, null, function* () {
      return new Promise((res, rej) => {
        this.instance.tokenize(params.content, params.nCtx, (response) => {
          if (response.type === TokenizeResultType.Data) {
            res(response.data);
          } else {
            rej(new Error("Unknown Error"));
          }
        });
      });
    });
  }
};
export {
  LLamaCpp
};
//# sourceMappingURL=llama-cpp.js.map