import { LlamaContextParams, LLama, LlamaInvocation } from '@llama-node/llama-cpp';
import { I as ILLM } from '../llm.d-120996aa.js';

interface LoadConfig extends LlamaContextParams {
    path: string;
    enableLogging: boolean;
}
interface TokenizeArguments {
    content: string;
    nCtx: number;
}
declare class LLamaCpp implements ILLM<LLama, LoadConfig, LlamaInvocation, LlamaInvocation, TokenizeArguments> {
    instance: LLama;
    load(config: LoadConfig): void;
    createCompletion(params: LlamaInvocation, callback: (data: {
        token: string;
        completed: boolean;
    }) => void): Promise<boolean>;
    getEmbedding(params: LlamaInvocation): Promise<number[]>;
    getDefaultEmbedding(text: string): Promise<number[]>;
    tokenize(params: TokenizeArguments): Promise<number[]>;
}

export { LLamaCpp, LoadConfig, TokenizeArguments };
